{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1334e3eb-6a25-4141-a350-c4698cbb8770",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import pandas as pd\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torchvision.utils import make_grid, save_image\n",
    "from glob import glob\n",
    "import cv2\n",
    "import random\n",
    "\n",
    "import shutil\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchsummary import summary\n",
    "from torch import optim\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score,plot_confusion_matrix\n",
    "\n",
    "import time\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "90915749-bd87-498a-9649-8f5352a6c8b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed(seed = 42):\n",
    "    random.seed(seed) # python random seed 고정\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed) # os 자체의 seed 고정\n",
    "    np.random.seed(seed) # numpy seed 고정 \n",
    "    torch.manual_seed(seed) # torch seed 고정\n",
    "    torch.cuda.manual_seed(seed) # cudnn seed 고정\n",
    "    torch.backends.cudnn.deterministic = True # cudnn seed 고정(nn.Conv2d)\n",
    "    torch.backends.cudnn.benchmark = False # CUDA 내부 연산에서 가장 빠른 알고리즘을 찾아 수행\n",
    "\n",
    "## DataLoader worker에 대한 seed 설정\n",
    "def seed_worker(worker_id):\n",
    "    worker_seed = torch.initial_seed() % 2**32\n",
    "    numpy.random.seed(worker_seed)\n",
    "    random.seed(worker_seed)\n",
    "    \n",
    "seed()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39484847-020b-4a6f-975b-76a1d858b37d",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Model Load(ResNet50(pretrained O))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b13c3db4-f528-4605-b58f-e403412538fd",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torchvision import models\n",
    "import torch\n",
    "\n",
    "resnet50_pretrained = models.resnet50(pretrained=True)\n",
    "print(resnet50_pretrained)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddf6d765-e9d5-48e8-95ef-ac9678762998",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "num_classes = 1\n",
    "num_features = resnet50_pretrained.fc.in_features\n",
    "resnet50_pretrained.fc = nn.Linear(num_features, num_classes)\n",
    "\n",
    "resnet50_pretrained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "980bb983-a862-4697-bba7-a14d1e715e45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpu?  True\n",
      "Current gpu:  0\n",
      "Allocated: 0.0 GB\n",
      "Cached:    0.0 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/crop2292/anaconda3/envs/hoon/lib/python3.6/site-packages/torch/cuda/memory.py:386: FutureWarning: torch.cuda.memory_cached has been renamed to torch.cuda.memory_reserved\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2906],\n",
      "        [0.1736],\n",
      "        [0.0941]], device='cuda:0', grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "GPU_NUM = 0\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]='1'\n",
    "print('gpu? ', torch.cuda.is_available())\n",
    "device = torch.device(f'cuda:{GPU_NUM}' if torch.cuda.is_available() else 'cpu')\n",
    "torch.cuda.set_device(device)\n",
    "print('Current gpu: ', torch.cuda.current_device())\n",
    "\n",
    "if device.type == 'cuda':\n",
    "    print('Allocated:', round(torch.cuda.memory_allocated(GPU_NUM)/1024**3,1), 'GB')\n",
    "    print('Cached:   ', round(torch.cuda.memory_cached(GPU_NUM)/1024**3,1), 'GB')\n",
    "    \n",
    "model = resnet50_pretrained.to(device)\n",
    "x = torch.randn(3, 3, 224, 224).to(device)\n",
    "output = model(x)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9955aef1-cbaa-4ead-a1a4-61ffb84fe8b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_name = 'Resnet50(b=16,Adam,Focal_alpha(0.75),WRS,sche,seed)_weights_pt'\n",
    "model_name = 'resnet50(b=16,Adam,Focal_alpha(0.65),WRS,sche,seed,7,pre,AUG,re)_weights_pt'\n",
    "model_path = '/mnt/nas100_vol2/LeeJungHoon/AOV_task(binary_clssification)/Model_V2/weights_file/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "802b8214-935b-46b9-ae13-1c153256cacd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = resnet50_pretrained.to(device)\n",
    "model.load_state_dict(torch.load(model_path + model_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "11562fdb-36a2-4d06-8ce7-8c14e45756e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/mnt/nas100_vol2/LeeJungHoon/AOV_task(binary_clssification)/Model_V2/Grad_CAM_code'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e5122258-3647-4e4f-8158-ad6a9cbcaf0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "결과 저장 위치:  /mnt/nas100_vol2/LeeJungHoon/AOV_task(binary_clssification)/Model_V2/Grad-CAM_results/Grad-CAM_results(resnet50(b=16,Adam,Focal_alpha(0.65),WRS,sche,seed,7,pre,AUG,re))\n"
     ]
    }
   ],
   "source": [
    "# import datetime\n",
    "# current_time = datetime.datetime.now() + datetime.timedelta(hours= 9)\n",
    "# current_time = current_time.strftime('%Y-%m-%d-%H:%M')\n",
    "\n",
    "\n",
    "saved_loc = os.path.join('/mnt/nas100_vol2/LeeJungHoon/AOV_task(binary_clssification)/Model_V2/Grad-CAM_results/Grad-CAM_results(resnet50(b=16,Adam,Focal_alpha(0.65),WRS,sche,seed,7,pre,AUG,re))', )\n",
    "if os.path.exists(saved_loc):\n",
    "    shutil.rmtree(saved_loc)\n",
    "os.mkdir(saved_loc)\n",
    "\n",
    "print(\"결과 저장 위치: \", saved_loc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d2f2cc1-104f-4bdf-9eae-1378182796eb",
   "metadata": {},
   "source": [
    "## test dataset & test dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9052ec3a-63d5-434e-86fc-d7e56b63deb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_normal_path = '/mnt/nas100_vol2/LeeJungHoon/Aov_task_curation_data/curation_data_v4/data_v4_7_1/normal_train_v4/*.jpg'\n",
    "train_abnormal_path ='/mnt/nas100_vol2/LeeJungHoon/Aov_task_curation_data/curation_data_v4/data_v4_7_1/abnormal_train_v4/*.jpg'\n",
    "valid_normal_path ='/mnt/nas100_vol2/LeeJungHoon/Aov_task_curation_data/curation_data_v4/data_v4_7_1/normal_val_v4/*.jpg'\n",
    "valid_abnormal_path ='/mnt/nas100_vol2/LeeJungHoon/Aov_task_curation_data/curation_data_v4/data_v4_7_1/abnormal_val_v4/*.jpg'\n",
    "test_normal_path ='/mnt/nas100_vol2/LeeJungHoon/Aov_task_curation_data/curation_data_v4/data_v4_7_1/normal_test_v4/*.jpg'\n",
    "test_abnormal_path ='/mnt/nas100_vol2/LeeJungHoon/Aov_task_curation_data/curation_data_v4/data_v4_7_1/abnormal_test_v4/*.jpg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dd0baa95-76b5-499c-ae50-2a40d9632742",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_normal : 419\n",
      "val_normal : 95\n",
      "test_normal : 76\n",
      "------------------------------------\n",
      "train_abnormal : 352\n",
      "val_abnormal : 75\n",
      "test_abnormal : 78\n"
     ]
    }
   ],
   "source": [
    "train_normal_glob = glob(train_normal_path)\n",
    "train_abnormal_glob = glob(train_abnormal_path)\n",
    "val_normal_glob = glob(valid_normal_path)\n",
    "val_abnormal_glob = glob(valid_abnormal_path)\n",
    "test_normal_glob = glob(test_normal_path)\n",
    "test_abnormal_glob = glob(test_abnormal_path)\n",
    "\n",
    "print('train_normal :', len(train_normal_glob))\n",
    "print('val_normal :', len(val_normal_glob))\n",
    "print('test_normal :', len(test_normal_glob))\n",
    "print('------------------------------------')\n",
    "print('train_abnormal :', len(train_abnormal_glob))\n",
    "print('val_abnormal :', len(val_abnormal_glob))\n",
    "print('test_abnormal :', len(test_abnormal_glob))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f42d6c8b-427c-4346-8b62-d6719ccec9b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Aov_Dysplasia_dataset(Dataset):\n",
    "    def __init__(self, normal_path, abnormal_path, transform=None):\n",
    "        #생성자, 데이터를 전처리 \n",
    "        self.normal_path_list = glob(normal_path)\n",
    "        self.abnormal_path_list = glob(abnormal_path)\n",
    "        # print(len(self.normal_path_list))\n",
    "#         self.mode = mode \n",
    "    \n",
    "#         label = np.array([[0, 1], [1, 0]], dtype=np.float32)\n",
    "        \n",
    "#         self.label_list = []\n",
    "#         for i in self.normal_path_list:\n",
    "#             self.label_list.append(label[0])\n",
    "            \n",
    "#         for i in self.abnormal_path_list:\n",
    "#             self.label_list.append(label[1])\n",
    "            \n",
    "        label_policy = {\n",
    "            'normal': 0, \n",
    "            'abnormal': 1\n",
    "        }\n",
    "    \n",
    "        self.label_list= []\n",
    "        \n",
    "        for i in self.normal_path_list:\n",
    "            self.label_list.append(label_policy[\"normal\"])\n",
    "            \n",
    "        for i in self.abnormal_path_list:\n",
    "            self.label_list.append(label_policy[\"abnormal\"])\n",
    "        \n",
    "        self.total_img_path_list = self.normal_path_list + self.abnormal_path_list\n",
    "        print(len(self.total_img_path_list))\n",
    "        self.transform = transform\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.total_img_path_list)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        img = cv2.imread(self.total_img_path_list[idx])\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        # img = np.array(img, dtype=np.float32)\n",
    "        #들어오는 이미지의 컬러 형태가 BGR인지 RGB인지 모르기때문에 변형\n",
    "\n",
    "        \n",
    "        label = self.label_list[idx]\n",
    "        \n",
    "        if self.transform is not None:\n",
    "            transformed = self.transform(image=img)\n",
    "            img = transformed['image'] \n",
    "            img = torch.tensor(np.array(img), dtype=torch.float32)\n",
    "            # img = torch.FloatTensor(img)\n",
    "            img = (img - torch.min(img)) / (torch.max(img)-torch.min(img))\n",
    "            \n",
    "            return {'img': img, 'label': label, 'filename': self.total_img_path_list[idx]}\n",
    "        \n",
    "        else:\n",
    "            # img = transformed['image']\n",
    "            img = torch.tensor(np.array(img), dtype=torch.float32)\n",
    "            # img = torch.FloatTensor(img)\n",
    "            img = (img - torch.min(img)) / (torch.max(img)-torch.min(img))\n",
    "            return{'img': img, 'label': label}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f848d606-c2f5-4af8-b4de-0af8e7ae6c15",
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://albumentations.ai/docs/api_reference/augmentations/transforms/\n",
    "import albumentations as A \n",
    "from  albumentations.pytorch import ToTensorV2\n",
    "\n",
    "train_transform = A.Compose(\n",
    "    [\n",
    "        # Contrast Limited Adaptive Histogram Equalization 적용\n",
    "#     A.CLAHE(p=1,clip_limit=(1, 3)),\n",
    "#     A.HorizontalFlip(p=0.3),\n",
    "    A.Resize(224,224, interpolation = cv2.INTER_AREA),\n",
    "    A.ShiftScaleRotate(shift_limit=0, scale_limit=(0.1, 0.2), rotate_limit=0, p=0.6, border_mode=cv2.BORDER_REPLICATE),\n",
    "    A.CLAHE(clip_limit=(1, 2), p=0.6),\n",
    "    A.RandomRotate90(p=0.7),\n",
    "    A.VerticalFlip(p=0.7),\n",
    "    A.HorizontalFlip(p=0.7),\n",
    "    A.RandomBrightnessContrast(brightness_limit=(-0.2, 0.2), contrast_limit=(-0.2, 0.2), p=0.3),\n",
    "    # A.Normalize()\n",
    "    ToTensorV2()\n",
    "    ])\n",
    "\n",
    "valid_transform = A.Compose(\n",
    "    [\n",
    "        A.Resize(224,224, interpolation = cv2.INTER_AREA),\n",
    "        ToTensorV2()\n",
    "    ])\n",
    "\n",
    "test_transform = A.Compose(\n",
    "    [\n",
    "        A.Resize(224,224, interpolation = cv2.INTER_AREA),\n",
    "        ToTensorV2()\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bf8e364b-9740-43d0-a48d-a483bc280ee1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "771\n",
      "170\n",
      "154\n"
     ]
    }
   ],
   "source": [
    "train_dataset = Aov_Dysplasia_dataset(train_normal_path, train_abnormal_path, transform = train_transform)\n",
    "valid_dataset = Aov_Dysplasia_dataset(valid_normal_path, valid_abnormal_path, transform = valid_transform)\n",
    "test_dataset = Aov_Dysplasia_dataset(test_normal_path, test_abnormal_path, transform = test_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6c37a4c6-5a55-4a55-9afb-3a079b3e6fa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=1, shuffle = False ,worker_init_fn=seed_worker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fe186307-62ce-447c-80d1-d528a07bd0b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "154"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_dataloader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d910b70c-e2f2-45e3-ab9b-5b5feb634af2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# final conv layer name \n",
    "# feature map을 추출할 layer를 설정\n",
    "finalconv_name = 'layer4'\n",
    "\n",
    "# inference mode\n",
    "model.eval()\n",
    "\n",
    "# number of result\n",
    "# num_result = 10\n",
    "\n",
    "\n",
    "feature_blobs = []\n",
    "\n",
    "backward_feature = []\n",
    "\n",
    "# output으로 나오는 feature를 feature_blobs에 append하도록\n",
    "def hook_feature(module, input, output):\n",
    "    feature_blobs.append(output.cpu().data.numpy())\n",
    "    \n",
    "# Grad-CAM\n",
    "def backward_hook(module, input, output):\n",
    "    backward_feature.append(output[0])\n",
    "\n",
    "    \n",
    "model._modules.get(finalconv_name).register_forward_hook(hook_feature)\n",
    "model._modules.get(finalconv_name).register_backward_hook(backward_hook)\n",
    "\n",
    "\n",
    "# get the sigmoid weight\n",
    "params = list(model.parameters())\n",
    "weight_sigmoid = np.squeeze(params[-2].cpu().detach().numpy()) # [1, 512]\n",
    "\n",
    "\n",
    "# generate the class activation maps\n",
    "def returnCAM(feature_conv, weight_simoid, class_idx):\n",
    "    size_upsample = (224, 224)\n",
    "    _, nc, h, w = feature_conv.shape # nc : number of channel, h: height, w: width\n",
    "    output_cam = []\n",
    "    # weight 중에서 class index에 해당하는 것만 뽑은 다음, 이를 conv feature와 곱연산\n",
    "    cam = weight_sigmoid[class_idx].dot(feature_conv.reshape((nc, h*w))) \n",
    "    cam = cam.reshape(h, w)\n",
    "    cam = cam - np.min(cam)\n",
    "    cam_img = cam / np.max(cam)\n",
    "    cam_img = np.uint8(255 * cam_img)\n",
    "    output_cam.append(cv2.resize(cam_img, size_upsample))\n",
    "    return output_cam\n",
    "\n",
    "\n",
    "incorrect_abnormal_list =[]\n",
    "incorrect_normal_list = []\n",
    "\n",
    "for data in test_dataloader:\n",
    "    \n",
    "    inputs = data['img'].float()\n",
    "    labels = data['label']\n",
    "    idx_list = data['filename']\n",
    "    idx = ' '.join(s for s in idx_list)\n",
    "    idx = idx.split('/')[-1]\n",
    "    \n",
    "    # 모델의 input으로 주기 위한 image는 따로 설정\n",
    "    image_for_model = inputs.clone().detach()\n",
    "\n",
    "    # Image denormalization, using mean and std that i was used.\n",
    "#     image[0][0] *= 0.2257\n",
    "#     image[0][1] *= 0.2209\n",
    "#     image[0][2] *= 0.2212\n",
    "    \n",
    "#     image[0][0] += 0.4876\n",
    "#     image[0][1] += 0.4544\n",
    "#     image[0][2] += 0.4165\n",
    "    \n",
    "\n",
    "    # 모델의 input으로 사용하도록.\n",
    "    image_tensor = image_for_model.to(device)\n",
    "    logit = model(image_tensor)\n",
    "    output = torch.squeeze(logit)\n",
    "    output_sig = torch.sigmoid(output)\n",
    "    y_pred = output_sig.cpu()\n",
    "    print(y_pred)\n",
    "    y_pred[y_pred >= 0.5] = 1\n",
    "    y_pred[y_pred < 0.5] = 0\n",
    "    \n",
    "    if y_pred != labels.cpu():\n",
    "        \n",
    "        if labels.cpu() == 1:\n",
    "            incorrect_abnormal_list.append(idx)\n",
    "        else:\n",
    "            incorrect_normal_list.append(idx)\n",
    "    label_out = labels.item()\n",
    "    y_pred_out = y_pred.item()\n",
    "    print(\"True label : %d, Predicted label : %d, idx : %s\" % (label_out, y_pred_out, idx))\n",
    "    \n",
    "    # ============================= #\n",
    "    # ==== Grad-CAM main lines ==== #\n",
    "    # ============================= #\n",
    "    \n",
    "    score = logit.squeeze() # 예측값 y^c\n",
    "    score.backward(retain_graph = True) # 예측값 y^c에 대해서 backprop 진행\n",
    "    \n",
    "    activations = torch.Tensor(feature_blobs[0]).to(device) # (1, 512, 7, 7), forward activations\n",
    "    gradients = backward_feature[0] # (1, 512, 7, 7), backward gradients\n",
    "    b, k, u, v = gradients.size()\n",
    "    \n",
    "    # view() 함수에서 -1은 다른 dimension에서 자동적으로 추론되는 것을 의미한다. \n",
    "    alpha = gradients.view(b, k, -1).mean(2) # (1, 512, 7*7) => (1, 512), feature map k의 'importance'\n",
    "    weights = alpha.view(b, k, 1, 1) # (1, 512, 1, 1)\n",
    "    \n",
    "    #위에서 지정해준 layer에서의 output인 activations과 backward gradients를 평균한 값인 weights를 곱해준다.\n",
    "    grad_cam_map = (weights*activations).sum(1, keepdim = True) # alpha * A^k = (1, 512, 7, 7) => (1, 1, 7, 7)\n",
    "    \n",
    "    # Apply R e L U\n",
    "    grad_cam_map = F.relu(grad_cam_map) \n",
    "    \n",
    "    grad_cam_map = F.interpolate(grad_cam_map, size=(224, 224), mode='bilinear', align_corners=False) # (1, 1, 224, 224)\n",
    "    map_min, map_max = grad_cam_map.min(), grad_cam_map.max()\n",
    "    grad_cam_map = (grad_cam_map - map_min).div(map_max - map_min).data # (1, 1, 224, 224), min-max scaling\n",
    "\n",
    "    # grad_cam_map.squeeze() : (224, 224)\n",
    "    grad_heatmap = cv2.applyColorMap(np.uint8(255 * grad_cam_map.squeeze().cpu()), cv2.COLORMAP_JET) # (224, 224, 3), numpy \n",
    "    grad_heatmap = torch.from_numpy(grad_heatmap).permute(2, 0, 1).float().div(255) # (3, 224, 224)\n",
    "    b, g, r = grad_heatmap.split(1)\n",
    "    grad_heatmap = torch.cat([r, g, b]) # (3, 244, 244), opencv's default format is BGR, so we need to change it as RGB format.\n",
    "\n",
    "    # save_image(grad_heatmap, os.path.join(saved_loc, \"%d_%d_%s\" % (label_out, y_pred_out, idx_out)))\n",
    "    \n",
    "    # print(grad_heatmap.type)\n",
    "    # print(inputsinputs.cpu().type)\n",
    "    grad_result = grad_heatmap + inputs.cpu() # (1, 3, 224, 224)\n",
    "    # print(grad_result.shape)\n",
    "    grad_result = grad_result.div(grad_result.max()).squeeze() # (3, 224, 224)\n",
    "    \n",
    "    # save_image(grad_result, os.path.join(saved_loc, \"GradCAM&image_%d.jpg\" % (xhch_idx+1)))\n",
    "    \n",
    "    \n",
    "    image_list = []\n",
    "    \n",
    "    image_list.append(torch.stack([inputs.squeeze().cpu(), grad_result], 0)) # (3, 3, 224, 224)\n",
    "    \n",
    "    images = make_grid(torch.cat(image_list, 0), nrow = 3)\n",
    "    \n",
    "    # image 저장\n",
    "    save_image(images, os.path.join(saved_loc, \"%d_%d_%s\" % (label_out, y_pred_out, idx)))\n",
    "    \n",
    "    # if  batch_idx + 1 == num_result:\n",
    "    #     break\n",
    "        \n",
    "    feature_blobs.clear()\n",
    "    backward_feature.clear()\n",
    "\n",
    "feature_blobs.clear()\n",
    "backward_feature.clear()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ef31cce-68ad-4ad8-9167-2c26ab5a5903",
   "metadata": {},
   "source": [
    "## validation dataset에 대한 Grad-CAM 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "481ee107-da2a-4844-9413-7243901796ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "결과 저장 위치:  /mnt/nas100_vol2/LeeJungHoon/AOV_task(binary_clssification)/Model_V2/Grad-CAM_results/Grad-CAM_results(Resnet_Focal(0.75)_WRS_sch_validation)\n"
     ]
    }
   ],
   "source": [
    "# import datetime\n",
    "# current_time = datetime.datetime.now() + datetime.timedelta(hours= 9)\n",
    "# current_time = current_time.strftime('%Y-%m-%d-%H:%M')\n",
    "\n",
    "\n",
    "saved_loc = os.path.join('/mnt/nas100_vol2/LeeJungHoon/AOV_task(binary_clssification)/Model_V2/Grad-CAM_results/Grad-CAM_results(Resnet_Focal(0.75)_WRS_sch_validation)', )\n",
    "if os.path.exists(saved_loc):\n",
    "    shutil.rmtree(saved_loc)\n",
    "os.mkdir(saved_loc)\n",
    "\n",
    "print(\"결과 저장 위치: \", saved_loc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ed74e5cf-e070-46e7-99eb-928d03437019",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_dataloader = torch.utils.data.DataLoader(valid_dataset, batch_size=1, shuffle = False, worker_init_fn=seed_worker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fca08dba-c1ff-428d-b474-138938b5ec98",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# final conv layer name \n",
    "# feature map을 추출할 layer를 설정\n",
    "finalconv_name = 'layer4'\n",
    "\n",
    "# inference mode\n",
    "model.eval()\n",
    "\n",
    "# number of result\n",
    "# num_result = 10\n",
    "\n",
    "\n",
    "feature_blobs = []\n",
    "\n",
    "backward_feature = []\n",
    "\n",
    "# output으로 나오는 feature를 feature_blobs에 append하도록\n",
    "def hook_feature(module, input, output):\n",
    "    feature_blobs.append(output.cpu().data.numpy())\n",
    "    \n",
    "# Grad-CAM\n",
    "def backward_hook(module, input, output):\n",
    "    backward_feature.append(output[0])\n",
    "\n",
    "    \n",
    "model._modules.get(finalconv_name).register_forward_hook(hook_feature)\n",
    "model._modules.get(finalconv_name).register_backward_hook(backward_hook)\n",
    "\n",
    "\n",
    "# get the sigmoid weight\n",
    "params = list(model.parameters())\n",
    "weight_sigmoid = np.squeeze(params[-2].cpu().detach().numpy()) # [1, 512]\n",
    "\n",
    "\n",
    "# generate the class activation maps\n",
    "def returnCAM(feature_conv, weight_simoid, class_idx):\n",
    "    size_upsample = (224, 224)\n",
    "    _, nc, h, w = feature_conv.shape # nc : number of channel, h: height, w: width\n",
    "    output_cam = []\n",
    "    # weight 중에서 class index에 해당하는 것만 뽑은 다음, 이를 conv feature와 곱연산\n",
    "    cam = weight_sigmoid[class_idx].dot(feature_conv.reshape((nc, h*w))) \n",
    "    cam = cam.reshape(h, w)\n",
    "    cam = cam - np.min(cam)\n",
    "    cam_img = cam / np.max(cam)\n",
    "    cam_img = np.uint8(255 * cam_img)\n",
    "    output_cam.append(cv2.resize(cam_img, size_upsample))\n",
    "    return output_cam\n",
    "\n",
    "\n",
    "incorrect_abnormal_list =[]\n",
    "incorrect_normal_list = []\n",
    "\n",
    "for data in valid_dataloader:\n",
    "    \n",
    "    inputs = data['img'].float()\n",
    "    labels = data['label']\n",
    "    idx_list = data['filename']\n",
    "    idx = ' '.join(s for s in idx_list)\n",
    "    idx = idx.split('/')[-1]\n",
    "    \n",
    "    # 모델의 input으로 주기 위한 image는 따로 설정\n",
    "    image_for_model = inputs.clone().detach()\n",
    "\n",
    "    # Image denormalization, using mean and std that i was used.\n",
    "#     image[0][0] *= 0.2257\n",
    "#     image[0][1] *= 0.2209\n",
    "#     image[0][2] *= 0.2212\n",
    "    \n",
    "#     image[0][0] += 0.4876\n",
    "#     image[0][1] += 0.4544\n",
    "#     image[0][2] += 0.4165\n",
    "    \n",
    "\n",
    "    # 모델의 input으로 사용하도록.\n",
    "    image_tensor = image_for_model.to(device)\n",
    "    logit = model(image_tensor)\n",
    "    output = torch.squeeze(logit)\n",
    "    output_sig = torch.sigmoid(output)\n",
    "    y_pred = output_sig.cpu()\n",
    "    print(y_pred)\n",
    "    y_pred[y_pred >= 0.5] = 1\n",
    "    y_pred[y_pred < 0.5] = 0\n",
    "    \n",
    "    if y_pred != labels.cpu():\n",
    "        \n",
    "        if labels.cpu() == 1:\n",
    "            incorrect_abnormal_list.append(idx)\n",
    "        else:\n",
    "            incorrect_normal_list.append(idx)\n",
    "    label_out = labels.item()\n",
    "    y_pred_out = y_pred.item()\n",
    "    print(\"True label : %d, Predicted label : %d, idx : %s\" % (label_out, y_pred_out, idx))\n",
    "    \n",
    "    # ============================= #\n",
    "    # ==== Grad-CAM main lines ==== #\n",
    "    # ============================= #\n",
    "    \n",
    "    score = logit.squeeze() # 예측값 y^c\n",
    "    score.backward(retain_graph = True) # 예측값 y^c에 대해서 backprop 진행\n",
    "    \n",
    "    activations = torch.Tensor(feature_blobs[0]).to(device) # (1, 512, 7, 7), forward activations\n",
    "    gradients = backward_feature[0] # (1, 512, 7, 7), backward gradients\n",
    "    b, k, u, v = gradients.size()\n",
    "    \n",
    "    # view() 함수에서 -1은 다른 dimension에서 자동적으로 추론되는 것을 의미한다. \n",
    "    alpha = gradients.view(b, k, -1).mean(2) # (1, 512, 7*7) => (1, 512), feature map k의 'importance'\n",
    "    weights = alpha.view(b, k, 1, 1) # (1, 512, 1, 1)\n",
    "    \n",
    "    #위에서 지정해준 layer에서의 output인 activations과 backward gradients를 평균한 값인 weights를 곱해준다.\n",
    "    grad_cam_map = (weights*activations).sum(1, keepdim = True) # alpha * A^k = (1, 512, 7, 7) => (1, 1, 7, 7)\n",
    "    \n",
    "    # Apply R e L U\n",
    "    grad_cam_map = F.relu(grad_cam_map) \n",
    "    \n",
    "    grad_cam_map = F.interpolate(grad_cam_map, size=(224, 224), mode='bilinear', align_corners=False) # (1, 1, 224, 224)\n",
    "    map_min, map_max = grad_cam_map.min(), grad_cam_map.max()\n",
    "    grad_cam_map = (grad_cam_map - map_min).div(map_max - map_min).data # (1, 1, 224, 224), min-max scaling\n",
    "\n",
    "    # grad_cam_map.squeeze() : (224, 224)\n",
    "    grad_heatmap = cv2.applyColorMap(np.uint8(255 * grad_cam_map.squeeze().cpu()), cv2.COLORMAP_JET) # (224, 224, 3), numpy \n",
    "    grad_heatmap = torch.from_numpy(grad_heatmap).permute(2, 0, 1).float().div(255) # (3, 224, 224)\n",
    "    b, g, r = grad_heatmap.split(1)\n",
    "    grad_heatmap = torch.cat([r, g, b]) # (3, 244, 244), opencv's default format is BGR, so we need to change it as RGB format.\n",
    "\n",
    "    # save_image(grad_heatmap, os.path.join(saved_loc, \"%d_%d_%s\" % (label_out, y_pred_out, idx_out)))\n",
    "    \n",
    "    # print(grad_heatmap.type)\n",
    "    # print(inputsinputs.cpu().type)\n",
    "    grad_result = grad_heatmap + inputs.cpu() # (1, 3, 224, 224)\n",
    "    # print(grad_result.shape)\n",
    "    grad_result = grad_result.div(grad_result.max()).squeeze() # (3, 224, 224)\n",
    "    \n",
    "    # save_image(grad_result, os.path.join(saved_loc, \"GradCAM&image_%d.jpg\" % (xhch_idx+1)))\n",
    "    \n",
    "    \n",
    "    image_list = []\n",
    "    \n",
    "    image_list.append(torch.stack([inputs.squeeze().cpu(), grad_heatmap, grad_result], 0)) # (3, 3, 224, 224)\n",
    "    \n",
    "    images = make_grid(torch.cat(image_list, 0), nrow = 3)\n",
    "    \n",
    "    # image 저장\n",
    "    save_image(images, os.path.join(saved_loc, \"%d_%d_%s\" % (label_out, y_pred_out, idx)))\n",
    "    \n",
    "    # if  batch_idx + 1 == num_result:\n",
    "    #     break\n",
    "        \n",
    "    feature_blobs.clear()\n",
    "    backward_feature.clear()\n",
    "\n",
    "feature_blobs.clear()\n",
    "backward_feature.clear()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0379b17d-f555-44de-b652-74d366f83162",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
